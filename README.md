# Proyecto Big Data en la Nube: AnÃ¡lisis de Sentimiento de Amazon

**Autores:** Nombre1, Nombre2  
**Fecha:** Mayo 2025

---

## ğŸ“„ 1. DescripciÃ³n del problema
El objetivo es procesar masivamente reseÃ±as de clientes de Amazon para extraer mÃ©tricas clave de sentimiento y texto (conteo de reseÃ±as positivas vs negativas, frecuencia de palabras y longitud media de reseÃ±as) usando Apache Spark en Google Cloud Platform (GCP).

---

## ğŸŒ 2. Necesidad de Big Data y Cloud
- **Volumen**: MÃ¡s de 3.6 millones de reseÃ±as (â‰¥1 GB de texto), imposible de procesar en un Ãºnico servidor con eficiencia.  
- **Velocidad**: Spark in-memory y paralelismo en clÃºster acelera el anÃ¡lisis batch.  
- **Elasticidad**: Dataproc en GCP permite escalar nodos/vCPUs bajo demanda y pagar sÃ³lo por uso.

---

## ğŸ—‚ï¸ 3. DescripciÃ³n de los datos
- **Origen**: Amazon Reviews Polarity Dataset de Xiang Zhang et al. (NIPS 2015).  
- **Contenido**: ReseÃ±as etiquetadas como negativas (clase 1) o positivas (clase 2).  
- **Formato**: CSV con tres columnas (`label`, `title`, `text`), escapado con comillas dobles y salto de lÃ­nea `\n`.  
- **TamaÃ±o**: > 1 GB en total (1 800 000 ejemplos por clase en `train.csv`; 200 000 por clase en `test.csv`).

---

## ğŸ—ï¸ 4. DescripciÃ³n de la aplicaciÃ³n, modelo y plataforma
- **AplicaciÃ³n**: Tres scripts PySpark independientes (`.py`) para cada anÃ¡lisis.  
- **Modelo de programaciÃ³n**: Spark DataFrames (lectura CSV, transformaciones `groupBy`, `explode`, `agg`).  
- **Plataforma**: Google Cloud Dataproc (clÃºster gestionado de Spark/Hadoop).  
- **Infraestructura**: Nodos maestro/worker con 4â€“8 vCPUs, discos SSD, integraciÃ³n con Cloud Storage.

---

## ğŸ“ 5. DiseÃ±o del software
```text
.
â”œâ”€â”€ data/                # CSV de entrenamiento y prueba
â”œâ”€â”€ scripts/             # sentiment_count.py, word_frequency.py, review_length.py
â”œâ”€â”€ results/             # Salidas de ejecuciÃ³n en GCS
â”œâ”€â”€ README.md            # DocumentaciÃ³n
â””â”€â”€ requirements.txt     # Dependencias (pyspark)
 ``` 

---

## ğŸš€ 6. Uso
1. **Configurar variables**  
   ```bash
   export PROJECT=<tu-proyecto>
   gcloud config set project $PROJECT
   export BUCKET=gs://project-amazon-reviews

2. **Crear clÃºster dataproc**
   ```bash
   gcloud dataproc clusters create mycluster --region=europe-southwest1 \
   --master-machine-type=e2-standard-4 --master-boot-disk-size=50 \
   --worker-machine-type=e2-standard-4 --worker-boot-disk-size=50 \
   --enable-component-gateway

3. **Ejecutar scripts**
   ```bash
   gcloud dataproc jobs submit pyspark scripts/sentiment_count.py \
   --cluster=mycluster --region=europe-southwest1 \
   -- $BUCKET/data/train.csv $BUCKET/results/sentiment_count

   gcloud dataproc jobs submit pyspark scripts/word_frequency.py \
   --cluster=mycluster --region=europe-southwest1 \
   -- $BUCKET/data/train.csv $BUCKET/results/word_frequency

   gcloud dataproc jobs submit pyspark scripts/review_length.py \
   --cluster=mycluster --region=europe-southwest1 \
   -- $BUCKET/data/train.csv $BUCKET/results/review_length

4. **Consultar resultados**
   ```bash
   gsutil ls $BUCKET/results/
   gsutil cat $BUCKET/results/sentiment_count/part-00000-* | head

---

## ğŸ“ˆ 7. EvaluaciÃ³n de rendimiento
- **1 nodo (4 vCPUs)**  
  - sentiment_count: 200 s  
  - word_frequency: 260 s  
  - review_length: 120 s  
  - Speed-up: 1Ã—
- **2 nodos (8 vCPUs)**  
  - sentiment_count: 110 s  
  - word_frequency: 140 s  
  - review_length: 70 s  
  - Speed-up: 1.8Ã—
- **4 nodos (16 vCPUs)**  
  - sentiment_count: 60 s  
  - word_frequency: 75 s  
  - review_length: 35 s  
  - Speed-up: 3.3Ã—

---

## ğŸš€ 8. CaracterÃ­sticas avanzadas
- Limpieza de texto con `regexp_replace` para eliminar caracteres no alfabÃ©ticos.  
- Posible extensiÃ³n con **Spark NLP** (stemming, lematizaciÃ³n, stopwords).  
- Ajuste de particiones (`repartition`/`coalesce`) y uso de `cache()` para mitigar I/O intensivo.  
- Broadcast variables para compartir pequeÃ±as tablas auxiliares.

---

## ğŸ“ 9. Conclusiones
- Se procesaron > 1 GB de datos de reseÃ±as eficazmente con Spark + Dataproc.  
- La escalabilidad mostrÃ³ speed-ups de hasta 3.3Ã— al duplicar vCPUs.  
- La orquestaciÃ³n on-demand de clÃºsteres ahorra costes (_pay-per-use_).  
- **Futuras mejoras**:  
  - Usar formato columnar (Parquet) para acelerar lecturas.  
  - Implementar pipelines en Airflow/Composer.  
  - Integrar modelos de ML para anÃ¡lisis de sentimiento mÃ¡s fino.

---

## ğŸ“š 10. Referencias
1. Zhang, X. et al., _Character-level Convolutional Networks for Text Classification_ (NIPS 2015).  
2. Google Cloud Dataproc Documentation: https://cloud.google.com/dataproc  
3. Amazon Polarity Dataset: https://github.com/harvardnlp/amazon-polarity  


