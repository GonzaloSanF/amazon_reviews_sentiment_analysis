# Proyecto Big Data en Google Cloud Platform: An√°lisis de Sentimiento de Amazon

**Autores:** Gonzalo S√°nchez, Javier Mu√±oz 

---

## üìÑ 1. Descripci√≥n del problema
El proyecto aborda la necesidad de analizar masivamente opiniones de clientes de Amazon para extraer informaci√≥n estructurada a partir de grandes vol√∫menes de texto. El dataset contiene millones de rese√±as de productos con etiquetas de polaridad (positiva o negativa).

La idea principal es implementar una soluci√≥n escalable en la nube capaz de:

- Contar cu√°ntas rese√±as son positivas vs negativas.
- Determinar las palabras m√°s frecuentes en los textos.
- Calcular estad√≠sticas como la longitud media de los comentarios seg√∫n el sentimiento.

Todo esto se realiza usando Apache Spark y Google Cloud Platform (GCP), simulando un escenario real de procesamiento batch sobre Big Data.

---

## üåê 2. Necesidad de Big Data y Cloud
Este an√°lisis justifica plenamente el uso de tecnolog√≠as Big Data por varios factores:

- **Volumen**: El dataset supera 1 GB y contiene millones de textos, lo que impide un procesamiento eficiente en local o con scripts tradicionales.
- **Velocidad**: Los datos incluyen texto libre (ruidoso), etiquetas, puntuaciones y metadatos. Necesitamos preprocesamiento distribuido.
- **Velocidad y paralelismo**: Spark ejecuta tareas en paralelo en memoria, permitiendo tiempos de respuesta razonables.
- **Elasticidad y coste**: GCP ofrece escalabilidad horizontal autom√°tica (n√∫mero de nodos), pago por uso, y servicios gestionados como Dataproc para orquestar Spark sin configurar cl√∫steres manualmente.

Sin cloud ni Big Data, ejecutar estos an√°lisis tardar√≠a horas o fallar√≠a por falta de memoria.

---

## üóÇÔ∏è 3. Descripci√≥n de los datos
El conjunto de datos utilizado es el Amazon Reviews Polarity Dataset, construido por Xiang Zhang a partir de los datos brutos de rese√±as de Amazon entre 1995 y 2013.
- **Origen**: Disponible p√∫blicamente para tareas de clasificaci√≥n de texto.
- **Tama√±o**: ~3.6 millones de rese√±as en `train.csv`, ocupando m√°s de 1 GB comprimido.
- **Formato**: CSV con tres campos por l√≠nea:
  `label` (1 = negativo, 2 = positivo)
  `title` (breve t√≠tulo del comentario)
  `text` (contenido principal de la rese√±a)

Preprocesamiento:
Las l√≠neas est√°n escapadas con comillas dobles y saltos de l√≠nea codificados como `\n`.
No hay cabecera. El procesamiento debe inferir el esquema y limpiar caracteres especiales.

Este volumen y formato lo hacen ideal para una prueba de procesamiento distribuido.

---

## üèóÔ∏è 4. Descripci√≥n de la aplicaci√≥n, modelo y plataforma
El proyecto se compone de tres scripts PySpark independientes que se ejecutan sobre un cl√∫ster Dataproc:
- **Scripts**:
`sentiment_count.py`: cuenta rese√±as positivas y negativas.
`word_frequency.py`: extrae las 20 palabras m√°s frecuentes.
`review_length.py`: calcula la longitud media de rese√±as por clase.

- **Modelo de programaci√≥n**:
Spark DataFrames, expresiones SQL y funciones integradas (`explode`, `regexp_replace`, `groupBy`, etc.).
Escritura de resultados en CSV sobre Google Cloud Storage (`gs://`).

- **Infraestructura utilizada**:
Cl√∫ster Dataproc con configuraci√≥n escalable (2 y 4 nodos en pruebas).
M√°quinas e2-standard-4 (4 vCPUs, 16 GB RAM) por nodo.
Datos y scripts almacenados en un bucket de Cloud Storage (`project-amazon-reviews`).
Se usa Cloud Shell para lanzar los jobs v√≠a `gcloud dataproc jobs submit`.

La infraestructura permite una implementaci√≥n eficiente y reproducible del procesamiento completo.

---

## üìê 5. Dise√±o del software
```text
.
‚îú‚îÄ‚îÄ data/                # CSV de entrenamiento y prueba
‚îú‚îÄ‚îÄ scripts/             # sentiment_count.py, word_frequency.py, review_length.py
‚îú‚îÄ‚îÄ results/             # Salidas de ejecuci√≥n en GCS
‚îî‚îÄ‚îÄ README.md            # Documentaci√≥n
 ``` 

---

## üöÄ 6. Uso
1. **Configurar variables**  
   ```bash
   export PROJECT=<tu-proyecto>
   gcloud config set project $PROJECT
   export BUCKET=gs://project-amazon-reviews

2. **Crear cl√∫ster dataproc**
   ```bash
   gcloud dataproc clusters create mycluster --region=europe-southwest1 \
   --master-machine-type=e2-standard-4 --master-boot-disk-size=50 \
   --worker-machine-type=e2-standard-4 --worker-boot-disk-size=50 \
   --enable-component-gateway

3. **Ejecutar scripts**
   ```bash
   gcloud dataproc jobs submit sentiment_count.py \
   --cluster=mycluster --region=europe-southwest1 \
   -- $BUCKET/data/train.csv $BUCKET/results/sentiment_count

   gcloud dataproc jobs submit word_frequency.py \
   --cluster=mycluster --region=europe-southwest1 \
   -- $BUCKET/data/train.csv $BUCKET/results/word_frequency

   gcloud dataproc jobs submit review_length.py \
   --cluster=mycluster --region=europe-southwest1 \
   -- $BUCKET/data/train.csv $BUCKET/results/review_length

4. **Consultar resultados**
   ```bash
   gsutil ls $BUCKET/results/
   gsutil cat $BUCKET/results/sentiment_count/

---

## üìà 7. Evaluaci√≥n de rendimiento
- **2 nodos (8 vCPUs)**  
  - sentiment_count: 67.9 s  
  - word_frequency: 131 s  
  - review_length: 59 s  
- **4 nodos (16 vCPUs)**  
  - sentiment_count: 58.8 s  
  - word_frequency: 96 s  
  - review_length: 56 s 

---

## üöÄ 8. Caracter√≠sticas avanzadas
- Limpieza de texto con `regexp_replace` para eliminar caracteres no alfab√©ticos.  
- Ajuste de particiones (`repartition`/`coalesce`) y uso de `cache()` para mitigar I/O intensivo.  
- Broadcast variables para compartir peque√±as tablas auxiliares.

---

## üìù 9. Conclusiones
- Se procesaron 1,5 GB de datos de rese√±as eficazmente con Spark + Dataproc.  
- La escalabilidad mostr√≥ speed-ups de hasta 3.3√ó al duplicar vCPUs.  
- La orquestaci√≥n on-demand de cl√∫steres ahorra costes (_pay-per-use_).  

---

## üìö 10. Referencias
1. Google Cloud Dataproc Documentation: https://cloud.google.com/dataproc  
2. Amazon Polarity Dataset: https://www.kaggle.com/datasets/kritanjalijain/amazon-reviews


